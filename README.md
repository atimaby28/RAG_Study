# RAG Study

현직자들과 진행한 RAG(Retrieval-Augmented Generation) 스터디 내용을 정리한 레포지토리입니다.

**교재**: 테디노트의 랭체인을 활용한 RAG 비법노트 - 기본편  
**담당 챕터**: CHAPTER 12. 벡터 스토어(Vector Store)
**기간**: 12월 1일 ~ 12월 21일, 주 1회 (일요일 21시)

<p align="center">
  <img src="https://github.com/user-attachments/assets/6bed4dd1-6cb2-49ff-9f41-65baee81e980" width="30%">
  <img src="https://github.com/user-attachments/assets/66d01cf2-3ace-438b-9deb-5752645f6240" width="30%">
  <img src="https://github.com/user-attachments/assets/0a121a2a-603e-491c-81d9-aad9f7b30982" width="30%">
</p>



---

## RAG(Retrieval-Augmented Generation)란?

RAG는 최신 정보를 효과적으로 활용하기 위해 외부 정보를 참조하는 방식입니다. 기존 모델의 사전 학습된 데이터에만 의존하지 않고, 실시간으로 정보를 검색하여 적용합니다. 검색(Retrieval)과 생성(Generation)을 결합하여, AI가 자체 지식뿐만 아니라 외부 데이터에서 근거를 찾아 답변을 생성하도록 설계되었습니다.

---

## 생성형 AI의 한계와 RAG의 필요성

대규모 언어 모델(LLM)은 방대한 데이터를 학습하여 뛰어난 자연어 생성 능력을 보여주지만, 여러 한계점도 존재합니다. 가장 심각한 문제는 **환각(hallucination)** 현상입니다. GPT, Claude 등 최신 모델도 이 문제에서 완전히 자유롭지 못하며, 특히 특정 도메인 지식이나 최신 정보를 다룰 때 오류 발생 빈도가 높습니다.

### LLM의 주요 한계

1. **정적 지식의 한계**: 학습 시점 이후의 정보를 반영하지 못합니다. 예를 들어 2023년에 학습된 모델은 2025년의 연구 결과나 최신 시장 데이터를 제공할 수 없습니다.

2. **맞춤형 데이터 부족**: 기업 내부 문서나 도메인 특화 데이터는 학습 데이터에 포함되기 어려워 실제 현장에서 활용하기 부족합니다.

3. **출처 추적 어려움**: 모델이 생성한 답변은 근거를 명확히 제시하지 못해 사용자가 직접 사실 여부를 확인해야 합니다.

4. **업데이트 비용 문제**: 새로운 지식을 반영하려면 모델 전체를 재학습해야 하며, 이는 막대한 자원과 비용이 필요합니다. 잘못 학습된 정보를 수정하기도 어렵습니다.

### 실제 사례

- Google Bard가 제임스 웹 우주망원경(JWST)에 관한 잘못된 정보를 답변해 기업 주가에 영향을 준 사례
- 일부 모델이 존재하지 않는 논문이나 판례를 인용해 법적 문제를 일으킨 사례

이러한 한계로 인해 LLM을 단독으로 사용하는 대신, 외부 지식을 검색하여 결합하는 방식이 필요해졌고, RAG가 핵심 해결책으로 부상했습니다.

### 환각(Hallucination)의 원인

환각은 단순한 오답이 아니라 LLM 신뢰성을 근본적으로 흔드는 문제입니다.

**환각의 유형**
- **내재적 환각(Intrinsic)**: 학습한 정보를 잘못 조합하거나 문맥을 왜곡할 때 발생
- **외재적 환각(Extrinsic)**: 지식 범위를 넘어선 질문에 대해 근거 없는 답변을 생성할 때 발생

최근 연구에서는 이를 **개방형 환각(Open-domain)**과 **폐쇄형 환각(Closed-domain)**으로 세분화하기도 합니다. 전자는 무제한 질문에서, 후자는 주어진 문맥 내에서도 사실과 다른 내용을 생성하는 경우를 의미합니다.

환각의 근본 원인은 확률적 언어 모델의 구조적 특성에 있습니다. LLM은 불완전한 확률 분포를 바탕으로 다음 단어를 예측하기 때문에, 불확실성이 클 때 빈칸을 억지로 채우려는 경향을 보입니다. 특히 Top-k 샘플링이나 Nucleus Sampling 같은 디코딩 전략을 사용하면 환각이 더 자주 발생합니다.

**환각 감소를 위한 최신 연구**
- **ReDeEP(2024)**: 생성된 답변이 실제 검색 문서 근거에 기반했는지를 판별하여, 검색 문서를 벗어난 환각 구절을 정밀하게 추적
- **LettuceDetect(2024)**: 토큰 단위 분류기를 활용해 각 단어가 근거에 기반했는지를 세밀히 구분, 기존 대비 F1 점수 14.8% 향상(최대 79.22%)

결과적으로 외부 지식을 검색하여 검증할 수 있는 RAG 구조가 환각 억제에 가장 효과적인 해법으로 자리잡고 있습니다.

---

## 단순 LLM과 RAG의 차이

단순 LLM과 RAG 시스템의 가장 큰 차이는 **지식 활용 방식**입니다.

### 전통적인 LLM
- 학습 시점에 매개변수에 저장된 정적 지식만으로 답변 생성
- 훈련 이후 등장한 새로운 사실이나 특정 도메인의 전문 지식 반영 어려움

### RAG 시스템
- 외부 지식 소스에서 관련 문서를 검색하여 LLM 입력으로 제공
- 더 시의성 있고 사실적인 응답 생성 가능

**비유**: 전통적인 LLM이 훈련된 재료만으로 요리하는 셰프라면, RAG는 필요할 때마다 새로운 재료를 가져와 요리를 완성하는 셰프입니다.

### 성능 비교

최근 비교 연구 결과:
- **사실적 정확도**: RAG 92% vs LLM 78%
- **환각 발생률**: RAG 8% vs LLM 22%
- **지식 최신성**: RAG 9/10점 vs LLM 6/10점

### 주요 차이점

| 항목 | LLM | RAG |
|------|-----|-----|
| 지식 소스 | 학습 시점 내재 지식 | 사전 지식 + 외부 문서 |
| 맥락 처리 | 입력 토큰 윈도우 범위 내 | 검색된 외부 맥락 주입 |
| 확장성 | 재훈련 필요 | 지식베이스 갱신만으로 가능 |
| 응답 품질 | 일반 상식에 강함 | 최신/특수 도메인 지식에 강함 |

RAG는 정보 검색이 필요한 질의, 사실 검증이 중요한 작업, 최신성이 핵심인 응용 분야에서 순수 LLM보다 우수한 성능을 보여줍니다.

### 오픈 도메인 vs 클로즈드 도메인 RAG

**오픈 도메인 RAG**
- 위키피디아, 웹 전체 같은 공개 지식 기반
- 일반적인 질의응답 처리
- 장점: 폭넓은 지식 접근성, 다양한 관점 제공
- 단점: 신뢰성 검증과 노이즈 필터링의 어려움
- 예시: Meta의 RAG 논문 - 전체 위키피디아를 백엔드로 사용

**클로즈드 도메인 RAG**
- 특정 기업 내부 문서, 의료 교과서, 법률 DB 등 제한된 지식 기반
- 장점: 높은 정확도와 보안성
- 단점: 제한된 지식 범위, 데이터 구축·관리 비용
- 예시: 기업용 RAG 비서, 의료 RAG 시스템

최근에는 **Multi-Meta-RAG 연구(2024)**처럼 데이터베이스 필터링과 LLM 기반 메타데이터 추출을 결합하여 멀티홉 쿼리 성능을 크게 개선한 사례도 있습니다.

---

## 검색과 생성의 결합

RAG의 핵심은 정보 검색(Retrieval)과 텍스트 생성(Generation)을 유기적으로 결합하는 것입니다.

### 전통적인 정보검색(IR) vs RAG
- **전통적 IR**: 문서 검색 결과를 단순히 나열
- **RAG**: 검색 결과를 LLM의 입력 맥락에 직접 통합하여 최종 응답 생성

### 작동 원리

1. **질문 입력**: 사용자가 질의(query) 입력
2. **검색**: 검색기(retriever)가 미리 구축된 문서 데이터베이스에서 관련성 높은 텍스트 조각 검색
3. **전달**: 검색된 텍스트 조각과 원래 질의를 함께 생성기(generator, LLM)에 전달
4. **생성**: 생성기가 내재적 지식과 검색된 외부 정보를 참고하여 정확하고 근거 있는 응답 생성

**비유**: 검색기는 필요한 자료를 찾아주는 사서, 생성기는 그 자료를 바탕으로 완성된 글을 쓰는 작가

### Dense 벡터 임베딩 기반 의미 검색

RAG에서 활용되는 검색은 단순 키워드 매칭이 아닌 **Dense 벡터 임베딩 기반 의미 검색**입니다.

- 트랜스포머 기반 언어 모델(BERT, Sentence-BERT 등)이 텍스트의 문맥적 의미를 벡터 공간으로 인코딩
- "데이터센터 보안" 질문에도 직접적인 키워드가 없는 "서버실 접근 제어", "클라우드 인프라 방화벽"이 포함된 문서를 관련 결과로 검색 가능
- 기존 키워드 검색보다 풍부한 의미적 연결성 제공

### 동적 상호작용

검색과 생성의 동적 상호작용이 RAG의 중요한 특징입니다.
- 검색 단계는 단순히 정보를 전달하는 것을 넘어 생성 단계의 응답 품질과 방향성을 결정
- 예시: 의료 분야에서 최신 임상 가이드라인을 검색하여 환자 맞춤 치료 계획 제시

### 출처 제공

RAG는 생성 과정에서 **참조한 문서 출처를 함께 제공**할 수 있어, 응답의 투명성과 신뢰성을 높입니다.

---

## RAG 프로세스

RAG는 단계별로 이어지는 파이프라인 구조를 따릅니다.

### 단일 질의 처리 과정

1. **데이터 인덱싱**
   - 외부 문서를 전처리하고 청킹(Chunking)
   - 벡터 임베딩으로 변환하여 벡터 데이터베이스에 저장
   - 검색 효율성을 좌우하는 핵심 준비 단계

2. **질의 전처리 및 임베딩**
   - 사용자 질문을 벡터로 변환
   - 질문의 의도 파악

3. **문서 검색**
   - 벡터 질의를 기반으로 데이터베이스에서 가장 유사도가 높은 상위 k개 문서 조각 검색
   - 코사인 유사도, Dot Product, L2 Distance 등의 지표 활용
   - 필요시 BM25 같은 전통적 sparse 검색이나 메타데이터 기반 필터링 병행

4. **재순위화(Re-ranking)** (선택 단계)
   - 검색 결과가 많을 경우, re-ranker 모델이 질문-문서 간 정밀 점수 계산
   - 최적의 컨텍스트 선별

5. **프롬프트 증강**
   - 선택된 문서 조각을 원래 질문과 결합하여 LLM 입력 프롬프트 구성

6. **응답 생성 및 후처리**
   - LLM이 증강된 프롬프트를 기반으로 답변 생성
   - 필요시 참조 문서 출처를 함께 제공
   - 요약이나 형식화 과정을 거쳐 사용자에게 전달

이 과정은 매 질의마다 실시간으로 수행되며, 각 단계의 품질이 최종 성능으로 이어집니다.

### 속도 vs 정확도 균형

RAG 시스템 설계에서 가장 큰 과제 중 하나는 **지연시간(latency)과 정확도(accuracy) 사이의 균형**입니다.

**기본 트레이드오프**
- 더 많은 문서 검색 + 정교한 분석 = 정확도 향상, 응답 시간 증가
- 검색 범위 축소 = 속도 향상, 정보 누락 가능성 증가

**각 단계의 성능 영향**

| 단계 | 속도 영향 | 정확도 영향 |
|------|----------|------------|
| 데이터 인덱싱 | 사전 작업 (실시간 영향 없음) | 전체 정확도 좌우 |
| 질의 임베딩 | 일정한 지연 발생 | - |
| 문서 검색 | k값 증가시 처리 시간 증가 | k값 증가시 정확도 향상 |
| 재순위화 | 추가 지연 발생 | 정확도 향상 |
| 프롬프트 증강 & 응답 생성 | 컨텍스트 길이에 비례 | 컨텍스트 길이에 비례 |

**설계시 고려사항**
- 애플리케이션 성격 (실시간 챗봇 vs 분석 보고서)
- 사용자 기대치 (즉시 응답 vs 정확한 답변)
- 도메인 특성 (일반 정보 vs 전문 지식)

RAG는 "완벽한 정답"보다는 주어진 제약 조건 내에서 "충분히 좋은 답변"을 빠르게 제공하는 것이 목표입니다.

---

## 데이터 소스와 문서 전처리

RAG 시스템의 성능은 어떤 데이터를 사용하고, 그 데이터를 얼마나 잘 전처리하느냐에 크게 좌우됩니다.

### 데이터 소스 유형

1. **정형 데이터**: 관계형 데이터베이스 테이블처럼 구조화된 정보
2. **반정형 데이터**: JSON, XML, CSV처럼 메타데이터가 포함된 문서
3. **비정형 데이터**: PDF, 워드 문서, 웹 페이지, 이미지, FAQ, 뉴스 기사, 기술 매뉴얼, 법률 판례 등 자연어 중심 자료

### 산업별 활용 사례

- **고객 지원**: FAQ, 채팅 로그
- **법률**: 판례, 법령 문서 (OCR 처리)
- **의료**: 임상 가이드라인, 학술 논문

중요한 점은 지식베이스가 해당 도메인의 질문을 충분히 커버하면서도 최신성을 유지해야 한다는 것입니다.

### 문서 전처리 과정

문서 전처리는 검색 정확도와 응답 품질을 좌우하는 핵심 단계입니다.

1. **텍스트 추출**: PDF, HTML, 스캔 이미지 등에서 본문 텍스트와 메타정보 추출
2. **정제(Cleaning)**: 불필요한 공백, 제어문자, 마크업 태그 제거 및 표준화
3. **분할(Chunking)**: 긴 문서를 작은 텍스트 조각으로 나누어 검색 효율성 확보
4. **메타데이터 태깅**: 작성자, 생성일, 출처, 카테고리 같은 속성 부여로 정밀 검색 지원

**전문 도구**
- Unstructured AI
- Document AI

최근에는 LLM 기반 지능형 메타데이터 추출 기법도 연구되고 있으며, 클라우드 플랫폼들은 자연어 쿼리로 메타데이터 필터를 자동 생성하는 기능도 제공합니다.

전처리된 문서 조각들은 인덱싱 단계에서 벡터화되어 벡터 데이터베이스에 저장됩니다. **데이터 수집 → 전처리 → 인덱싱 → 지속적 갱신**으로 이어지는 파이프라인을 얼마나 정교하게 설계하느냐가 RAG 아키텍처 구축의 출발점입니다.

---

## 벡터 임베딩과 벡터 데이터베이스

RAG의 핵심 기반은 텍스트를 기계가 다룰 수 있는 **고차원 벡터 표현(임베딩)**으로 변환하는 과정입니다.

### 벡터 임베딩

임베딩은 단어·문장·문서를 다차원 공간의 벡터로 변환하여, 의미적 유사성을 수학적으로 계산할 수 있도록 합니다.

**장점**
- 단순 키워드 매칭을 넘어 문맥과 의도까지 반영 가능
- 예시: "클라우드 보안" 질의가 임베딩 공간에서 "데이터 암호화", "접근 권한 관리"와 가까운 위치에 배치

**발전 과정**
- 초창기: 단어 수준 (Word2Vec, GloVe)
- 현재: 문장·문서 수준 (Sentence-BERT, OpenAI text-embedding-3 계열)

OpenAI의 text-embedding-3-small 모델은 하나의 단락을 1,536차원 벡터로 표현하여 대규모 의미 공간에서 비교할 수 있습니다.

RAG에서는 사용자의 질의와 문서 조각을 같은 임베딩 공간에 투영한 후, 두 벡터 간 유사성을 **근사 최근접 탐색(ANN)** 기법으로 효율적으로 계산합니다.

### 벡터 데이터베이스

벡터 DB는 수백만 개 이상의 고차원 벡터를 실시간으로 탐색할 수 있도록 최적화되어 있으며, 벡터와 함께 문서 ID와 메타데이터도 보관하여 결과와 출처를 함께 반환합니다.

#### 주요 벡터 데이터베이스

| 이름 | 특징 |
|------|------|
| **FAISS** | Meta 개발, GPU 가속으로 초당 수백만 쿼리 처리 가능, 인프라 관리 복잡 |
| **Pinecone** | 완전 관리형 서비스, 자동 확장, 100ms 미만 지연, 보안·컴플라이언스 지원 |
| **Weaviate** | 오픈소스, 하이브리드 검색(sparse+dense) 지원, 모듈화된 AI 통합, 100ms 미만 지연 |
| **Milvus** | 대규모 분산 처리 특화, 클라우드 네이티브 인프라, 높은 확장성 |
| **Annoy** | Spotify 개발, 메모리 효율적, 정적 데이터에 강점 |
| **ChromaDB** | 개발자 친화적, $18M 시드 펀딩, 단순성과 사용 편의성 |
| **Qdrant** | HNSW 알고리즘 기반, 고급 압축 기술, 4~6ms 초저지연 |
| **Redis** | 인메모리 DB + 벡터 검색 기능, 기존 서비스와 쉬운 연계 |

벡터 DB 선택 시 고려사항: 속도, 확장성, 보안, 하이브리드 검색 지원 여부

### 벡터 거리 측정 방식

벡터 검색에서 "얼마나 가까운지"를 계산하는 방식이 검색 품질을 좌우합니다.

**주요 방식**

1. **코사인 유사도(Cosine Similarity)**
   - 벡터 간 각도를 기준으로 방향의 유사성 평가
   - 벡터 크기 무시, 순수한 의미적 유사성에 집중
   - 텍스트 임베딩에서 가장 널리 사용

2. **Dot Product(내적)**
   - 벡터의 크기와 방향 모두 고려
   - 정규화된 벡터에서는 코사인 유사도와 같은 순위 제공
   - 계산 속도 빠름

3. **L2 Distance(유클리드 거리)**
   - 두 벡터 간 절대적인 직선 거리 계산
   - 정확한 위치 기반 계산에 적합
   - 고차원 텍스트 벡터에서는 효율성 저하 가능

**선택 기준**
- 대부분의 텍스트 임베딩 모델(Sentence-BERT 계열)은 코사인 유사도 기준으로 학습 → 검색에도 코사인 유사도 사용 권장
- 일부 특수 모델은 내적 최적화로 훈련 → dot product가 더 정확할 수 있음
- 벡터가 정규화되어 있다면 코사인과 내적의 순위 동일 → 속도를 위해 내적 선택 가능

**핵심**: 모델 학습 시 사용된 유사도 함수와 검색 시 사용하는 유사도 함수가 일치해야 모델이 학습한 벡터 공간의 의미 구조를 제대로 활용 가능

---

## 검색기(Retriever)와 재순위화(Re-ranker)

### 검색기(Retriever)

검색기는 RAG 시스템의 핵심 구성 요소로, 사용자의 질의와 관련된 문서를 벡터 데이터베이스에서 빠르고 효율적으로 찾아냅니다.

#### 희소 검색(Sparse Retrieval)
- **기법**: BM25, TF-IDF 같은 전통 IR 기법
- **원리**: 키워드 일치 기반 문서 검색
- **장점**: 특정 용어(예: "쿠버네티스")가 포함된 문서를 빠르고 정확하게 검색, 평균 응답 시간 100ms 미만
- **단점**: 동의어나 의미적 변형에 취약

#### 밀집 검색(Dense Retrieval)
- **기법**: 트랜스포머 기반 임베딩 모델
- **원리**: 질의와 문서를 고차원 벡터로 변환, 코사인 유사도나 내적으로 의미적 유사성 평가
- **장점**: 키워드가 달라도 의미상 가까운 문서 검색 가능
  - 예: "서버 확장성" 질의 → "인스턴스 스케일링", "오토스케일링" 매칭
- **단점**: 모델 품질에 크게 의존, 실시간 추론 비용 높음

### 재순위화(Re-ranker)

초기 검색으로 수백 개의 후보 문서를 추린 후, 그 중에서 진짜 관련성 높은 문서들을 선별하는 정교한 필터링 단계입니다. 단순한 유사도 점수만으로는 놓칠 수 있는 문맥적 관련성을 더 깊이 분석하여 최종 순위를 조정합니다.

### 하이브리드 검색

Dense와 Sparse 검색은 서로 보완적인 특성을 가지므로, 실무에서는 두 방식을 결합한 하이브리드 검색이 널리 활용됩니다.

**장점 결합**
- **Sparse의 강점**: 정확한 키워드 매칭, 고유명사나 전문용어에 강함
- **Dense의 강점**: 의미적 유사성, 동의어나 paraphrasing에 강함

**일반적인 구조**
1. **병렬 검색**: Dense와 Sparse 검색을 동시 실행하여 각각 후보 문서 추출
2. **결과 융합**: 두 결과를 적절한 방식으로 결합 (점수 가중합, 순위 기반 융합 등)
3. **재순위화**: 융합된 결과를 대상으로 최종 순위 조정

**설계시 고려사항**
- 정확한 팩트 검색이 중요 → Sparse 비중 높임
- 의미적 연관성이 중요 → Dense 비중 높임
- 응답 속도가 중요 → 단순한 결합 방식 선택

최근에는 여기에 LLM까지 포함하여 쿼리 리라이팅, 재순위화, 컨텍스트 최적화까지 보완하는 "지능형 하이브리드 구조"도 등장하고 있습니다.

---

## LLM 응답 생성기

LLM 응답 생성기는 RAG 파이프라인의 마지막 단계로, 검색된 컨텍스트와 사용자 질의를 결합하여 사실 기반이면서도 자연스러운 답변을 생성하는 핵심 모듈입니다.

### 주요 역할

- 검색된 자료를 이해하고 재구성하여 매끄러운 문장으로 전달
- 단순히 정보를 이어 붙이는 것이 아닌, 컨텍스트 기반 종합적 답변 생성

### 프롬프트 증강(Prompt Augmentation)

프롬프트 시작 부분에 "다음 정보를 참고해 질문에 답하라"라고 명시하면, 모델이 자체 지식보다 검색된 근거를 중심으로 답변을 구성합니다.

### 출처 인용(Citation)

최근 RAG 시스템들은 응답에 출처를 인용하는 방식을 적극 도입하고 있습니다.
- 결과 검증 가능
- 환각 발생 확률 감소
- 신뢰도 향상

### Context Filtering 기법

컨텍스트 필터링은 검색 단계에서 가져온 정보 중 질문과 직접적인 관련성이 낮거나 불필요한 노이즈를 제거하여 응답 품질을 높이는 핵심 절차입니다.

**필요성**
- 모든 검색 결과를 그대로 LLM에 입력하면:
  - 입력 길이 초과 가능
  - 모델의 집중력 저하
  - 무관한 문맥으로 인한 환각 유발

**대표적인 기법**

| 기법 | 설명 |
|------|------|
| 유사도 임계치 기반 필터링 | 질의와의 의미적 유사도가 일정 기준 이하인 문서 제외 |
| 요약 기반 필터링 | 각 문서를 요약하여 핵심 문장만 남김 |
| 청크 단위 LLM 필터링 (ChunkRAG) | 청크별 점수를 부여하여 낮은 점수의 텍스트 제거 |
| 신뢰도·적응형 필터링 | 질의 난이도에 따라 필터링 강도 조정 |
| Context Awareness Gate (CAG) | LLM이 스스로 외부 문맥 활용 여부 판단 |

**효과**
- 답변의 정확성과 일관성 향상
- 토큰 사용량 감소로 비용 효율성 개선
- 향후 강화학습 기반 최적화, 문맥 중요도 학습, 동적 정보 선택 등으로 발전 예상

---

## 문서 Chunking 전략

문서 청킹(Chunking)은 RAG 시스템의 검색 성능을 좌우하는 핵심 전처리 단계입니다. 청킹이 제대로 되지 않으면:
- 중요한 개념이 여러 조각으로 흩어짐
- 전혀 다른 내용이 한 덩어리에 묶임
- 검색 정확도 저하

### 주요 청킹 방법론

#### 1. 고정 크기 청킹(Fixed-Size Chunking)
- 일정 토큰 수나 문자 수 기준으로 균일하게 분할
- **장점**: 계산 비용 낮음, 구현 간단
- **단점**: 문장이나 단락 경계를 무시하여 의미 단절 가능

#### 2. 재귀적 문자 청킹(Recursive Character Chunking)
- 구두점이나 개행 문자를 우선적으로 고려하여 분할
- **장점**: 기본적인 문서 구조 보존, 단순 고정 크기보다 자연스러움

#### 3. 의미적 청킹(Semantic Chunking)
- LLM이나 임베딩을 활용하여 의미 단위로 분할
- **장점**: 학술 논문, 기술 보고서 같은 복잡한 문서에서 효과적
- **단점**: 계산 비용 높음

#### 4. 구조 기반 청킹(Document Structure-Based Chunking)
- 문서 자체의 형식 활용
- **예시**: 마크다운은 헤더 레벨, HTML은 태그 구조 기준
- **장점**: 계층적 컨텍스트 유지

### 청크 크기가 성능에 미치는 영향

**작은 청크 (128~256 토큰)**
- **장점**: 검색 정밀도 높음, 질의와 정확히 매칭되는 작은 정보 단위 검색에 유리
- **단점**: 문맥 부족으로 종합적인 답변 생성 어려움

**큰 청크 (800~1200 토큰)**
- **장점**: 풍부한 문맥 정보로 완전한 답변 생성 가능
- **단점**: 관련 없는 정보가 섞여 검색 정확도 저하 가능

### 도메인별 일반적 경향

| 문서 유형 | 최적 청크 크기 | 이유 |
|----------|--------------|------|
| 뉴스 기사 | 작은 청크 | 짧고 간결한 정보 단위 |
| 학술 논문 | 큰 청크 | 복잡하고 연결된 개념 |
| 기술 문서 | 중간 크기 | 단계별 설명이 많음 |
| 법률/금융 문서 | 구조 기반 청킹 | 구조화된 형식 |

### 균형점 찾기

**실무 전략**
- **오버랩 적용**: 청크 간 일정 부분을 겹치게 하여 문맥 단절 방지
- **유동적 크기**: 문서 구조에 따라 청크 크기 조정
- **계층적 청킹**: 큰 청크와 작은 청크를 함께 사용하여 장점 결합

최적 청크 크기는 도메인, 문서 구조, 질의 패턴에 따라 달라지며, 각자의 상황에 맞는 전략을 찾는 과정이 필요합니다.

---

## 메타데이터 설계

메타데이터(metadata)는 단순히 문서에 붙는 태그가 아니라, RAG 시스템의 검색 정확도와 응답 품질을 좌우하는 전략적 자산입니다.

### 주요 메타데이터 필드

- 문서 출처 (URL, 문서명 등)
- 저자, 작성·발행일자
- 문서 종류 (뉴스, 블로그, 논문 등)
- 주제 분류, 부서/프로젝트 구분
- 보안 등급, 중요도, 신뢰도 점수

### 활용 방식

메타데이터는 단순한 분류를 넘어 검색 재순위화나 응답 생성에도 활용됩니다.
- 동일한 답변 후보가 있으면 "공식 문서" 우선 제공
- 자동으로 답변에 출처 인용

### 동적 메타데이터 추출

LLM이 질의에서 연도, 조직, 주제 같은 속성을 추출하고, 이를 검색 필터로 변환합니다.

**예시**: "2025년 데이터센터의 전력 효율성 개선 방안은?"
→ 자동으로 연도=2025, 분야=데이터센터 조건 부여

### 계층적 메타데이터 구조

기업 환경에서 자주 활용되며, 부서, 프로젝트, 보안 등급 같은 다층 필드를 통해 접근 제어와 맥락 기반 검색을 동시에 구현할 수 있습니다.

최신 벡터 데이터베이스(Supabase, Pinecone 등)는 이러한 복합 필터링을 기본 지원합니다.

### 쿼리 시점 필터링(Query-time Filtering)

검색 실행 시점에 메타데이터 조건을 적용하여 불필요한 문서를 사전에 제거하는 고급 기법입니다.

**장점**
- 벡터 유사도 계산 전에 후보군 축소
- 효율성과 정확도 동시 향상

**구현 방식**
- 자연어 질의에서 시기·주제·도메인 같은 속성 자동 추출
- Function Calling 방식으로 동적 필터 구성
- 예시: "최근 2년간 클라우드 보안 동향" → 시기=2023~2025, 분야=클라우드보안

**하이브리드 아키텍처**
- pgvector: TimeScale 하이퍼테이블을 활용하여 시계열 필터링과 벡터 검색을 단일 SQL 쿼리로 처리
- 시간 민감형 정보 검색에서 성능 크게 개선

### 메타데이터 필터 최적화 전략

- **선택적 필터링**: 복잡한 질의일 때만 필터 적용
- **캐싱 메커니즘**: 자주 사용되는 필터 조합 저장 및 재사용
- **점진적 필터링**: 검색 과정에서 필터 강도를 단계적으로 강화
- **동적 임계값 조정**: 질의 복잡도에 따라 필터 수준 유연하게 조절

---

## RAG 동작 예시 (단일 쿼리)

RAG 시스템의 단일 쿼리 처리는 일반적인 LLM과 달리 외부 지식 검색을 통합하여 최신성과 정확성을 확보합니다.

### 예시 질의

1. "2024년 데이터센터 시장 성장률"
2. "2025년 클라우드 신기술"

### 처리 과정

#### 1. 질의 전처리 및 벡터화

- 사용자 질의를 임베딩 모델을 거쳐 벡터로 변환
  - 예: OpenAI text-embedding-3-small 모델이 문장을 1,536차원 벡터로 인코딩
- 메타데이터 추출기가 연도, 주제 같은 속성을 인식하여 검색 필터 조건 설정
  - "2024년 데이터센터 시장 성장률" → 연도=2024, 주제=데이터센터 시장
  - "2025년 클라우드 신기술은 무엇인가?" → 연도=2025, 주제=클라우드 신기술

#### 2. 벡터 데이터베이스 검색

- 생성된 질의 벡터를 벡터 DB에 저장된 문서 벡터와 비교
- Dense 검색: 의미적 유사도 평가
- Sparse 검색: 키워드 매칭 (예: "데이터센터", "성장률", "클라우드", "신기술")
- 하이브리드 방식으로 두 방식 동시 반영
- 예: IDC의 데이터센터 시장 보고서, 2025년 클라우드 기술 전망 기사가 상위 후보로 선정

#### 3. 재순위화 및 컨텍스트 선정

- 초기 검색 후보(수십~수백 개)를 re-ranker 모델로 압축
- Cohere Reranker 같은 모델이 문단이 질문에 얼마나 직접적으로 응답할 수 있는지 평가
- 상위 20개 내외로 압축
- 오래되거나 무관한 문서 제거
- 문단에 출처 표시
  - 예: "2024년 IDC 데이터센터 시장 보고서 발췌"
  - 예: "2025년 4월 Gartner 클라우드 기술 전망 기사"

#### 4. 프롬프트 증강 및 응답 생성

- 선정된 컨텍스트와 질문을 LLM 입력 프롬프트로 결합
- 프롬프트에 "다음 정보에 근거하여 답하라:" 같은 지시문 포함
- Claude, GPT 같은 생성 모델이 응답 생성

**응답 예시**
- 데이터센터 시장: "2024년 글로벌 데이터센터 시장은 전년 대비 18.2% 성장할 것으로 예측되며, 이는 IDC 보고서를 근거로 합니다."
- 클라우드 신기술: "2025년에는 클라우드 네이티브 보안, 무중단 멀티클라우드 오케스트레이션, AI 기반 자원 최적화가 핵심 신기술로 부상할 것으로 전망되며, 이는 Gartner 2025 클라우드 전망 보고서에 근거합니다."

#### 5. 신뢰성 확보

- 일반 LLM: 학습 종료 시점 이후의 최신 사실을 알지 못해 환각 발생 가능
- RAG: 외부 문서 검색 + 출처 인용을 통해 최신성과 신뢰성 확보

---

## 검색 실패 시 보완 전략

RAG 시스템에서 검색 실패는 피할 수 없는 상황입니다. 지식베이스에 문서가 없거나, 질의가 모호하거나, 벡터 검색의 한계 때문에 적절한 컨텍스트를 찾지 못하는 경우가 발생합니다.

### 1. 질의 확장 & 재검색

**다중 쿼리 재작성(Multi-Query Rewriting)**
- 검색 결과가 부족할 때 사용
- 예시: "AI 윤리 가이드라인" → "인공지능 윤리 원칙", "머신러닝 윤리 기준"

**HyDE(Hypothetical Document Embeddings)**
- 가상의 답변을 먼저 생성하고 이를 검색에 활용
- 실제 문서가 없어도 유사한 자료 검색 가능

### 2. 검색 모드 전환 & 가중치 조정

- Dense 검색 실패 시 Sparse 검색(BM25 등)으로 전환
- 하이브리드 검색의 가중치 조정
  - 예: sparse_boost 값을 1.2에서 2.0으로 변경하여 키워드 기반 검색 비중 증가

### 3. 메타데이터 필터 완화

- 조건이 너무 엄격하면 검색이 막힐 수 있음
- 예: "2025년" 시간 필터를 "2022~2025년"으로 확장
- 특정 카테고리 제한 해제로 검색 범위 확대

### 검색 실패 시 대안 전략

#### 1. Zero-shot 프롬프트 생성
- 외부 컨텍스트가 없을 때 LLM의 내재 지식만으로 답변 생성
- 반드시 불확실성 표현 포함 필요
  - "정확하지 않을 수 있습니다"
  - "관련 정보를 찾지 못했습니다"

#### 2. Few-shot 프롬프트 보완
- Zero-shot의 한계를 줄이기 위해 유사 질의-응답 예시 제공
- 예: "2025년 데이터센터 전력 효율성 개선률은?"
  → "구체적 수치는 확인되지 않았지만, 냉각 시스템과 서버 가상화 부문에서 높은 개선 효과가 보고되었습니다"

#### 3. 외부 API 연동
- 최신성과 실시간성이 중요한 질문(주가, 날씨, 최신 뉴스)은 외부 API와 연결
- API 호출을 RAG 파이프라인과 결합하는 도구 활발히 개발 중

### 계층적 보완 전략 구조

보완 전략은 보통 다음 순서로 작동합니다:

1. 기본 RAG 검색
2. 질의 재작성 기반 확장 검색
3. 검색 모드 전환
4. 메타데이터 조건 완화
5. LLM 내재 지식 활용
6. 외부 API 호출
7. 최종 "정보 부족" 응답

각 단계마다 검색 품질이 평가되고, 기준에 못 미치면 다음 단계로 진행합니다.

---

## RAG의 장점

### 1. 최신성

전통적인 LLM은 훈련 시점 이후의 지식을 반영하지 못하지만, RAG는 외부 지식베이스나 실시간 데이터 소스에 접근할 수 있습니다.

**활용 사례**
- 금융 분석 시스템: 최신 분기 실적 보고서 즉시 반영
- 의료 진단 보조 도구: 최신 임상지침 근거로 답변 제공

### 2. 정확성

단독 LLM은 추론 과정에서 환각을 일으킬 위험이 있지만, RAG는 외부의 신뢰 가능한 문서를 참조하여 훨씬 더 사실적인 답변을 생성합니다.

**실제 성과**
- 의료 분야 실험: RAG 기반 진단 시스템이 96.4% 정확도와 거의 0%의 환각률 기록 (의사 집단보다 높은 성능)
- DoorDash 운전자 지원 챗봇: 내부 정책 문서 근거로 일관성과 정확성 유지

### 3. 출처 추적성과 투명성

RAG는 생성된 답변에 구체적인 출처를 명시하여 사용자가 직접 신뢰성을 검증할 수 있습니다.

**중요성**
- 설명 가능한 AI(Explainable AI) 요구 충족
- 의료·법률·금융 등 오판 비용이 큰 분야에서 필수적
- 예: 금융 예측 시스템이 특정 전망의 근거가 된 재무제표나 시장 보고서 함께 제시

### 4. 맥락 강화와 사용자 맞춤화

질문의 의도와 맥락을 고려하여 응답을 조율합니다.

**예시**
- "그가 세운 법칙은 무엇인가?" → 대화 흐름과 관련 문서를 참조하여 '그'가 누구인지 해석하고 올바른 법칙 제시
- 개인 메모나 사내 위키 같은 지식 소스 연결로 사용자 상황에 맞춘 맞춤형 답변 가능

### 5. 비용 효율성과 확장성

전통적으로 LLM을 최신화하려면 모델 전체를 재훈련하거나 파인튜닝해야 했지만, RAG는 벡터 인덱싱과 검색 모듈만 업데이트하면 됩니다.

**장점**
- 새로운 데이터가 들어와도 모델 파라미터를 건드릴 필요 없음
- 안정성 높음
- 유지·운영 비용 크게 절감
- "대규모 모델 확장" 대신 "외부 지식 결합" 패러다임 전환

---

## RAG의 한계와 과제

### 1. 데이터 품질 의존성

RAG의 성능은 결국 지식베이스 품질에 달려 있습니다.

**위험 요소**
- 검색된 문서가 오래되었거나 잘못된 정보를 담고 있으면 "겉보기에 그럴듯하지만 틀린 답변" 생성
- 공개 웹 데이터를 그대로 사용하면 가짜 뉴스나 편향된 자료가 섞일 위험
- "garbage in, garbage out" 원리 적용

**해결책**: 데이터 수집·검증 과정에 충분한 투자와 관리 필요

### 2. 응답 지연과 시스템 복잡성

RAG는 단순 LLM보다 처리 단계가 훨씬 많습니다.

**지연 발생 요인**
- 임베딩, 벡터 검색, 재순위화, 컨텍스트 필터링 등 다단계 처리
- 연구 결과: 전체 응답 지연의 약 41%가 검색 과정, 93.2%가 벡터 생성에서 발생
- 실제 사례: 1초 내 응답하던 챗봇이 RAG 적용 후 2~3초 이상 소요
- 고객지원 환경에서 사용자 만족도 20% 감소 보고

**복잡성 증가**
- 검색 인덱스, 데이터 파이프라인, 프롬프트 설계 등 관리 요소 증가

### 3. 비용 및 인프라 부담

임베딩 계산, 벡터 DB 운영, 대규모 인덱스 유지에는 상당한 연산 자원이 필요합니다.

**비용 요인**
- Pinecone 같은 관리형 벡터 DB: 편리하지만 비용 높음
- FAISS 같은 오픈소스 솔루션: 인프라 관리 부담 큼
- 테라바이트 단위 데이터: 최적화 없이 저장하면 스토리지 비용 폭증
- 문서 청크 증가: 토큰 사용량 기하급수적 증가로 LLM 호출 비용 상승

**해결책**: 검색 최적화, 컨텍스트 필터링, 결과 캐싱 같은 비용 절감 전략 필수

### 4. 컨텍스트 창 한계

아무리 큰 모델이라도 컨텍스트 윈도우에는 한계가 있습니다.

**문제점**
- 모든 검색 결과를 LLM에 입력할 수 없음
- 중요한 정보가 잘려나갈 위험
- 요약이나 압축 기법 사용 시 정보 손실 위험
- 장문 보고서나 복합 질의에서 성능 저하

---

## RAG 성능 측정 지표

RAG 시스템의 성능을 객관적으로 평가하고 개선하려면 적절한 지표가 필요합니다. 검색과 생성이 함께 작동하는 RAG 특성상, 단순한 정확도 측정으로는 부족합니다.

### 주요 검색 품질 지표

#### Recall@K
- 상위 K개 검색 결과 중 실제 관련 문서가 포함될 비율
- RAG에서 특히 중요: 필요한 문서를 놓치는 순간 답변이 불완전하거나 왜곡될 위험

#### MRR(Mean Reciprocal Rank)
- 첫 번째 관련 문서가 얼마나 상위에 위치하는지 평가
- 예: 관련 문서가 3번째에 나타나면 RR = 1/3
- 상위권에 좋은 문서가 있을수록 LLM이 더 나은 답변을 생성할 가능성 증가

#### Precision@K
- 상위 K개 검색 결과 중 실제로 관련 있는 문서의 비율
- 불필요한 정보가 많이 섞이면 LLM이 혼란스러워질 수 있어 중요

### 중요성

검색 단계에서 실패하면 아무리 뛰어난 LLM이라도 좋은 답변을 만들 수 없습니다. RAG에서는 **검색 품질이 전체 시스템 성능의 상한선을 결정**합니다.

### 운영 비용 추적

성능 지표와 함께 다음 비용도 추적해야 합니다:
- 벡터 저장소 운영 비용
- 임베딩 생성 비용
- 검색 연산 비용
- LLM 추론 비용

RAG 시스템은 **성능과 비용 사이의 균형점을 찾는 것이 핵심**이며, 이러한 지표들을 통해 지속적으로 모니터링하고 개선해나가는 과정이 필요합니다.

---

## 참고 자료

- 교재: 테디노트의 랭체인을 활용한 RAG 비법노트 - 기본편
- 출처: [kt cloud 기술 블로그](https://tech.ktcloud.com/entry/2025-08-ktcloud-ai-rag-시스템구조-이해)
